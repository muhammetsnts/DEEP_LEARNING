{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Farklı bir classificcation problem ile karşılaştığında yapılacak düzeltmeler\n",
    "#1. .npz nin ismini değiştir.\n",
    "#2. classes_num ı değiştir.\n",
    "\n",
    "import numpy as np\n",
    "# Batching yapacak bir class yapıyoruz\n",
    "\n",
    "class Audiobooks_Data_Reader():\n",
    "    #Bu class iterator olacak. \n",
    "    \n",
    "    #init .npz dosyasındaki datayı yüklüyor.\n",
    "    def __init__(self,dataset,batch_size=None):\n",
    "        #Batch size optional. girilmezse none girilirse o olur. Validation ve test setlerinde batch e gerek olmadığından onlarda none olabilir.\n",
    "        #x('train',5) dediğimizde Audiobooks_data_train.npz yi batch size 5 olacak şekilde yükler.\n",
    "        \n",
    "        npz=np.load(\"Audiobooks_data_{0}.npz\".format(dataset))\n",
    "        self.inputs, self.targets= npz[\"inputs\"].astype(np.float), npz[\"targets\"].astype(np.int)\n",
    "        \n",
    "        #batch size verilmemişse tek batch yap dedik.\n",
    "        if batch_size is None:\n",
    "            self.batch_size=self.inputs.shape[0]\n",
    "        else:\n",
    "            self.batch_size=batch_size\n",
    "        self.curr_batch = 0\n",
    "        self.batch_count = self.inputs.shape[0] // self.batch_size\n",
    "    \n",
    "    #next methodu npz deki bir sonraki batchi yükler.\n",
    "    def __next__(self):\n",
    "        if self.curr_batch >= self.batch_count:\n",
    "            self.curr_batch = 0\n",
    "            raise StopIteration()\n",
    "        \n",
    "        #datayı batchlere böldük ve next bize bunları sırayla yüklemeyi sağlar.\n",
    "        batch_slice = slice(self.curr_batch * self.batch_size, (self.curr_batch + 1) * self.batch_size)#hangi parçaların alınacağı. (0*batch_size dan 1*batcsize a kadar böl diyor.)\n",
    "        inputs_batch = self.inputs[batch_slice]\n",
    "        targets_batch  = self.targets[batch_slice]\n",
    "        self.curr_batch +=1\n",
    "\n",
    "        #one-hot encode the targets. zaten 0-1 lerden oluşuyor ama sonraki kullanımlar için böyle yaptık.\n",
    "        classes_num = 2 #0 ve 1 olmak üzere 2 sınıfımız var daha fazla olursa ona göre yapılır.\n",
    "        targets_one_hot = np.zeros((targets_batch.shape[0], classes_num)) #one-hot ta 0-> [1,0] ve 1->[0,1] şeklinde gösterilecek \n",
    "        targets_one_hot[range(targets_batch.shape[0]), targets_batch] = 1\n",
    "        \n",
    "        #bu fonksiyon inputs batch ve one-hot encoded targets döndürür\n",
    "        return inputs_batch, targets_one_hot\n",
    "    \n",
    "    #Tüm batchleri tekrar etmek için bir method gerekir ki loopa sokalım\n",
    "    #Bu bize pythonda sınıfların iterable olduğunu söyler. şu şekilde kullanabiliriz yani:\n",
    "    #for input, output in data:\n",
    "    #  do things\n",
    "    #Pythonda iterasyon için kullanılan sınıf __next__ ile kullanılır çünkü next buna hangi objeleri iterate edceğini verir.\n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlining the Model Algorithm\n",
    "50 hidden unitli iki hiddn layer tanımlaycağız. input 10(10 sütunla işlem yapıyorduk) output 2 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Training loss:  0.700. Validation loss:  0.642. Validation accuracy: 69.57%\n",
      "Epoch: 2. Training loss:  0.630. Validation loss:  0.621. Validation accuracy: 61.74%\n",
      "Epoch: 3. Training loss:  0.593. Validation loss:  0.574. Validation accuracy: 70.92%\n",
      "Epoch: 4. Training loss:  0.557. Validation loss:  0.544. Validation accuracy: 77.18%\n",
      "Epoch: 5. Training loss:  0.529. Validation loss:  0.517. Validation accuracy: 75.17%\n",
      "Epoch: 6. Training loss:  0.503. Validation loss:  0.495. Validation accuracy: 73.60%\n",
      "Epoch: 7. Training loss:  0.481. Validation loss:  0.474. Validation accuracy: 75.39%\n",
      "Epoch: 8. Training loss:  0.462. Validation loss:  0.455. Validation accuracy: 77.85%\n",
      "Epoch: 9. Training loss:  0.446. Validation loss:  0.441. Validation accuracy: 77.85%\n",
      "Epoch: 10. Training loss:  0.433. Validation loss:  0.429. Validation accuracy: 78.08%\n",
      "Epoch: 11. Training loss:  0.422. Validation loss:  0.419. Validation accuracy: 79.19%\n",
      "Epoch: 12. Training loss:  0.412. Validation loss:  0.410. Validation accuracy: 78.97%\n",
      "Epoch: 13. Training loss:  0.404. Validation loss:  0.403. Validation accuracy: 78.30%\n",
      "Epoch: 14. Training loss:  0.396. Validation loss:  0.396. Validation accuracy: 78.08%\n",
      "Epoch: 15. Training loss:  0.390. Validation loss:  0.391. Validation accuracy: 78.75%\n",
      "Epoch: 16. Training loss:  0.385. Validation loss:  0.386. Validation accuracy: 78.75%\n",
      "Epoch: 17. Training loss:  0.380. Validation loss:  0.382. Validation accuracy: 78.30%\n",
      "Epoch: 18. Training loss:  0.375. Validation loss:  0.378. Validation accuracy: 78.52%\n",
      "Epoch: 19. Training loss:  0.371. Validation loss:  0.374. Validation accuracy: 78.75%\n",
      "Epoch: 20. Training loss:  0.367. Validation loss:  0.371. Validation accuracy: 78.97%\n",
      "Epoch: 21. Training loss:  0.364. Validation loss:  0.368. Validation accuracy: 78.97%\n",
      "Epoch: 22. Training loss:  0.361. Validation loss:  0.365. Validation accuracy: 78.97%\n",
      "Epoch: 23. Training loss:  0.358. Validation loss:  0.363. Validation accuracy: 79.19%\n",
      "Epoch: 24. Training loss:  0.356. Validation loss:  0.361. Validation accuracy: 79.19%\n",
      "Epoch: 25. Training loss:  0.354. Validation loss:  0.359. Validation accuracy: 79.19%\n",
      "Epoch: 26. Training loss:  0.351. Validation loss:  0.357. Validation accuracy: 79.19%\n",
      "Epoch: 27. Training loss:  0.349. Validation loss:  0.356. Validation accuracy: 79.19%\n",
      "Epoch: 28. Training loss:  0.348. Validation loss:  0.354. Validation accuracy: 79.42%\n",
      "Epoch: 29. Training loss:  0.346. Validation loss:  0.353. Validation accuracy: 78.97%\n",
      "Epoch: 30. Training loss:  0.344. Validation loss:  0.351. Validation accuracy: 78.97%\n",
      "Epoch: 31. Training loss:  0.343. Validation loss:  0.350. Validation accuracy: 78.97%\n",
      "Epoch: 32. Training loss:  0.341. Validation loss:  0.349. Validation accuracy: 78.97%\n",
      "Epoch: 33. Training loss:  0.340. Validation loss:  0.348. Validation accuracy: 79.42%\n",
      "Epoch: 34. Training loss:  0.338. Validation loss:  0.347. Validation accuracy: 79.19%\n",
      "Epoch: 35. Training loss:  0.337. Validation loss:  0.346. Validation accuracy: 79.19%\n",
      "Epoch: 36. Training loss:  0.336. Validation loss:  0.345. Validation accuracy: 79.19%\n",
      "Epoch: 37. Training loss:  0.335. Validation loss:  0.344. Validation accuracy: 79.19%\n",
      "Epoch: 38. Training loss:  0.334. Validation loss:  0.343. Validation accuracy: 79.19%\n",
      "Epoch: 39. Training loss:  0.333. Validation loss:  0.342. Validation accuracy: 79.19%\n",
      "Epoch: 40. Training loss:  0.332. Validation loss:  0.342. Validation accuracy: 79.87%\n",
      "Epoch: 41. Training loss:  0.331. Validation loss:  0.341. Validation accuracy: 80.09%\n",
      "Epoch: 42. Training loss:  0.330. Validation loss:  0.340. Validation accuracy: 80.09%\n",
      "Epoch: 43. Training loss:  0.329. Validation loss:  0.339. Validation accuracy: 80.09%\n",
      "Epoch: 44. Training loss:  0.328. Validation loss:  0.339. Validation accuracy: 80.09%\n",
      "Epoch: 45. Training loss:  0.327. Validation loss:  0.338. Validation accuracy: 80.09%\n",
      "Epoch: 46. Training loss:  0.326. Validation loss:  0.337. Validation accuracy: 80.09%\n",
      "Epoch: 47. Training loss:  0.325. Validation loss:  0.337. Validation accuracy: 80.09%\n",
      "Epoch: 48. Training loss:  0.324. Validation loss:  0.336. Validation accuracy: 80.09%\n",
      "Epoch: 49. Training loss:  0.324. Validation loss:  0.336. Validation accuracy: 80.31%\n",
      "Epoch: 50. Training loss:  0.323. Validation loss:  0.335. Validation accuracy: 80.31%\n",
      "Epoch: 51. Training loss:  0.322. Validation loss:  0.335. Validation accuracy: 80.54%\n",
      "Epoch: 52. Training loss:  0.322. Validation loss:  0.334. Validation accuracy: 81.21%\n",
      "Epoch: 53. Training loss:  0.321. Validation loss:  0.334. Validation accuracy: 81.21%\n",
      "Epoch: 54. Training loss:  0.320. Validation loss:  0.333. Validation accuracy: 80.98%\n",
      "Epoch: 55. Training loss:  0.320. Validation loss:  0.333. Validation accuracy: 80.98%\n",
      "Epoch: 56. Training loss:  0.319. Validation loss:  0.333. Validation accuracy: 80.98%\n",
      "Epoch: 57. Training loss:  0.319. Validation loss:  0.332. Validation accuracy: 80.98%\n",
      "Epoch: 58. Training loss:  0.318. Validation loss:  0.332. Validation accuracy: 80.98%\n",
      "Epoch: 59. Training loss:  0.317. Validation loss:  0.331. Validation accuracy: 80.98%\n",
      "Epoch: 60. Training loss:  0.317. Validation loss:  0.331. Validation accuracy: 80.98%\n",
      "Epoch: 61. Training loss:  0.316. Validation loss:  0.331. Validation accuracy: 80.98%\n",
      "Epoch: 62. Training loss:  0.316. Validation loss:  0.330. Validation accuracy: 80.98%\n",
      "Epoch: 63. Training loss:  0.315. Validation loss:  0.330. Validation accuracy: 80.98%\n",
      "Epoch: 64. Training loss:  0.315. Validation loss:  0.330. Validation accuracy: 80.98%\n",
      "Epoch: 65. Training loss:  0.315. Validation loss:  0.330. Validation accuracy: 80.98%\n",
      "Epoch: 66. Training loss:  0.314. Validation loss:  0.329. Validation accuracy: 81.21%\n",
      "Epoch: 67. Training loss:  0.314. Validation loss:  0.329. Validation accuracy: 81.21%\n",
      "Epoch: 68. Training loss:  0.313. Validation loss:  0.329. Validation accuracy: 81.21%\n",
      "Epoch: 69. Training loss:  0.313. Validation loss:  0.328. Validation accuracy: 81.21%\n",
      "Epoch: 70. Training loss:  0.312. Validation loss:  0.328. Validation accuracy: 80.98%\n",
      "Epoch: 71. Training loss:  0.312. Validation loss:  0.328. Validation accuracy: 81.21%\n",
      "Epoch: 72. Training loss:  0.312. Validation loss:  0.327. Validation accuracy: 81.21%\n",
      "Epoch: 73. Training loss:  0.311. Validation loss:  0.327. Validation accuracy: 81.43%\n",
      "Epoch: 74. Training loss:  0.311. Validation loss:  0.327. Validation accuracy: 81.43%\n",
      "Epoch: 75. Training loss:  0.311. Validation loss:  0.327. Validation accuracy: 81.43%\n",
      "Epoch: 76. Training loss:  0.310. Validation loss:  0.326. Validation accuracy: 81.43%\n",
      "Epoch: 77. Training loss:  0.310. Validation loss:  0.326. Validation accuracy: 81.43%\n",
      "Epoch: 78. Training loss:  0.310. Validation loss:  0.326. Validation accuracy: 81.88%\n",
      "Epoch: 79. Training loss:  0.309. Validation loss:  0.326. Validation accuracy: 81.88%\n",
      "Epoch: 80. Training loss:  0.309. Validation loss:  0.326. Validation accuracy: 81.88%\n",
      "Epoch: 81. Training loss:  0.309. Validation loss:  0.325. Validation accuracy: 81.88%\n",
      "Epoch: 82. Training loss:  0.308. Validation loss:  0.325. Validation accuracy: 81.66%\n",
      "Epoch: 83. Training loss:  0.308. Validation loss:  0.325. Validation accuracy: 81.66%\n",
      "Epoch: 84. Training loss:  0.308. Validation loss:  0.325. Validation accuracy: 81.66%\n",
      "Epoch: 85. Training loss:  0.308. Validation loss:  0.325. Validation accuracy: 81.66%\n",
      "Epoch: 86. Training loss:  0.307. Validation loss:  0.325. Validation accuracy: 81.66%\n",
      "Epoch: 87. Training loss:  0.307. Validation loss:  0.325. Validation accuracy: 81.66%\n",
      "Epoch: 88. Training loss:  0.307. Validation loss:  0.324. Validation accuracy: 81.66%\n",
      "Epoch: 89. Training loss:  0.307. Validation loss:  0.324. Validation accuracy: 82.10%\n",
      "Epoch: 90. Training loss:  0.306. Validation loss:  0.324. Validation accuracy: 82.10%\n",
      "Epoch: 91. Training loss:  0.306. Validation loss:  0.324. Validation accuracy: 82.10%\n",
      "Epoch: 92. Training loss:  0.306. Validation loss:  0.324. Validation accuracy: 82.10%\n",
      "Epoch: 93. Training loss:  0.306. Validation loss:  0.324. Validation accuracy: 82.10%\n",
      "Epoch: 94. Training loss:  0.306. Validation loss:  0.324. Validation accuracy: 82.10%\n",
      "Epoch: 95. Training loss:  0.305. Validation loss:  0.324. Validation accuracy: 82.10%\n",
      "Epoch: 96. Training loss:  0.305. Validation loss:  0.324. Validation accuracy: 82.10%\n",
      "Epoch: 97. Training loss:  0.305. Validation loss:  0.324. Validation accuracy: 82.10%\n",
      "Epoch: 98. Training loss:  0.305. Validation loss:  0.324. Validation accuracy: 82.10%\n",
      "overfitting!!!!!!!\n",
      "Training sonu\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#mnistte epoch loopa kadar olan kısmı kopyaladık buraya koyduk.\n",
    "\n",
    "input_size=10                              \n",
    "output_size=2\n",
    "hidden_layer_size=100\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#bu kod önceki run dan hafızada kalan değerleri resetlemek için kullanılır\n",
    "#w , b değerlerini güncelledikçe tekrar çalıştıracağımızdan buna ihtiyaç var.\n",
    "\n",
    "inputs=tf.placeholder(tf.float32,[None,input_size])\n",
    "targets=tf.placeholder(tf.int32,[None,output_size])\n",
    "\n",
    "#3 tane w ve b değeri olduğundan(2 hidden unit ve input output araları) ayrı ayrı giriyoruz\n",
    "weights_1=tf.get_variable(\"weights_1\",[input_size,hidden_layer_size]) #size ı 784*50\n",
    "biases_1=tf.get_variable(\"biases_1\",[hidden_layer_size])#size ı 50\n",
    "#değişkenleri atamanın geleneksel bir yoludur bu. ayrıca default olduğundan Xavier initializer kullanır.\n",
    "#name ve shape parametreleridir\n",
    "\n",
    "#burada hidden layer 1 değerlerini buluyoruz\n",
    "outputs_1=tf.nn.relu(tf.matmul(inputs,weights_1)+biases_1) # relu aktivasyon fonksiyonunu kullandık.(nonliearity)\n",
    "#tf.nn neural network ün kısaltması. yaygın kullanılan aktivasyonn fonksiyonlarını barındırır.\n",
    "\n",
    "#1. işlem tamam şimdi sıra 2.de\n",
    "\n",
    "weights_2=tf.get_variable(\"weights_2\",[hidden_layer_size,hidden_layer_size])\n",
    "biases_2=tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "\n",
    "outputs_2=tf.nn.sigmoid(tf.matmul(outputs_1,weights_2)+biases_2)\n",
    "\n",
    "weights_3=tf.get_variable(\"weights_3\", [hidden_layer_size,output_size])\n",
    "biases_3=tf.get_variable(\"biases_3\",[output_size])\n",
    "\n",
    "outputs=tf.matmul(outputs_2,weights_3)+biases_3\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs,labels=targets)\n",
    "\n",
    "#loss var ama mean loss ile çalışmak performansı artırır\n",
    "#kullanılan fonksiyon boyutlar halindeki tensorun ortalamasını bulur\n",
    "\n",
    "mean_loss=tf.reduce_mean(loss)\n",
    "\n",
    "#optimizason fonksiyonu belirlenir.öncekinde gd kullanmıştık ama artık adam kullanacağız\n",
    "\n",
    "optimize= tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)\n",
    "\n",
    "#output ile targetı karşılaştırıyoruz\n",
    "#tf.equal eşit mi değil mi değeri döndürür t/f\n",
    "#tf.argmax fonksiyonu içine verilen değer matrisinin neresinin karşılaştırılacağını alır.outpus,1 derse output matrisinin sütunu, output,0 derse satırı olur\n",
    "#out_equals_target da her gözlem için 0-1 lerden oluşan bir vektördür.\n",
    "#accuracy ise bu out_equals_target vektörünün meanidir.bu vektör boolean olduğundan sonucta hata verdirebilir. onun için bunu tf.cast ile floata çeviriyoruz\n",
    "#tf.cast(object,data_type) bir objeyi başka bir data tipine dönüştürmeye yarar\n",
    "\n",
    "out_equals_target=tf.equal(tf.argmax(outputs,1),tf.argmax(targets,1))\n",
    "\n",
    "accuracy=tf.reduce_mean(tf.cast(out_equals_target,tf.float32))\n",
    "# bu iki satır modelin training accuracy değerini 0-1 arasında olacak şekilde hesapladı.\n",
    "\n",
    "#session \n",
    "sess=tf.InteractiveSession()\n",
    "initializer=tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "##### BATCHİNG ########## BATCHİNGİ CLASS DA YAPIYORUZ BURADA SADECE SİZE VERİLDİ\n",
    "#batch size =1 olursa SGD olmuş olur. Batch size= number of samples olursa da GD olur.\n",
    "\n",
    "batch_size=500 #örnek sayısını batch size a bölünce batch sayısı çıkar\n",
    "\n",
    "########## EARLY STOPPING MEKANİZMALARI########\n",
    "\n",
    "#ilk olarak algoritmanın train edeceği maximum epoch sayısını giriyoruz\n",
    "#early stopping eğer validation loss artıyorsa \n",
    "\n",
    "max_epochs=1500 # makul zamanda bitirmesi için 15 girdik\n",
    "prev_validation_loss= 9999999 #yeterince büyük verdik ki 1. epochta hemen bitmesin diye\n",
    "\n",
    "########### VERİ YÜKLEME #############\n",
    "# mnistte veriyi yüklerken tensorflow data provider kullanmıştık. bize preprocessed vaziyette getirmişti.\n",
    "# ama şimdi kendimiz preprocess yaptık. bu yüzden biz yükleyeceğiz datayı.\n",
    "\n",
    "#Bunlar Audiobooks_data_reader classımızın birer nesnesi\n",
    "train_data = Audiobooks_Data_Reader('train', batch_size)\n",
    "validation_data = Audiobooks_Data_Reader('validation')\n",
    "\n",
    "for epoch_counter in range(max_epochs):\n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    ############# TRAINING ###########\n",
    "    \n",
    "    #class iterable olduğundan classı döngüye sokuyoruz\n",
    "    for input_batch, target_batch in train_data:\n",
    "        #bu sayede her seferinde train setten 100 er input ve output batchler alacağız\n",
    "        _, batch_loss=sess.run([optimize,mean_loss],\n",
    "            feed_dict={inputs:input_batch,targets:target_batch})\n",
    "        \n",
    "        curr_epoch_loss += batch_loss\n",
    "    \n",
    "     #mean curr_epoch hsaplanır\n",
    "    curr_epoch_loss /= train_data.batch_count #train_data.batch_count number of batches  \n",
    "    \n",
    "    ############ VALIDATING ##############\n",
    "    \n",
    "    #forward propagate yaparken loopa gerek yoktu aslında ama class iterator olduğundan böyle olmalı\n",
    "    #dolayısıyla bu döngü hep tek iterasyon yapar\n",
    "    for input_batch, target_batch in validation_data:\n",
    "        validation_loss, validation_accuracy = sess.run([mean_loss, accuracy],\n",
    "                                                feed_dict={inputs:input_batch, targets:target_batch})\n",
    "\n",
    "######## YAZDIRMA ############\n",
    "    \n",
    "    print(\"Epoch: \" +str(epoch_counter+1)+\n",
    "          \". Training loss: \"+\" {0:.3f}\".format(curr_epoch_loss)+\n",
    "          \". Validation loss: \"+\" {0:.3f}\".format(validation_loss)+\n",
    "          \". Validation accuracy: \"+\"{0:.2f}\".format(validation_accuracy*100.)+\"%\"\n",
    "         )\n",
    "    \n",
    "    \n",
    "    #### EARLY STOPPING MEKANİZMASI #########\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        print(\"overfitting!!!!!!!\")\n",
    "        break\n",
    "        \n",
    "    prev_validation_loss=validation_loss #böylece validation loss artmaya başlayınca duracak\n",
    "    \n",
    "print(\"Training sonu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 79.91%\n"
     ]
    }
   ],
   "source": [
    "test_data = Audiobooks_Data_Reader('test')\n",
    "\n",
    "for input_batch, target_batch in test_data:\n",
    "    test_accuracy=sess.run([accuracy],\n",
    "                      feed_dict={inputs:input_batch, targets:target_batch})\n",
    "# run ile bir single output yürütürsek sess.run sonucu tek elemanlı list olur\n",
    "\n",
    "test_accuracy_percent = test_accuracy[0]*100.\n",
    "\n",
    "print(\"Test accuracy: \" + \"{0:.2f}\".format(test_accuracy_percent)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
