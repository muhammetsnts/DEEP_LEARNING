{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time \n",
    "\n",
    "baslangic=time.time()\n",
    "#input data hazır training test ve validation olarak hazır 3 e bölünmüş gelir\n",
    "#ayrıca data preprocessed şekilde gelir\n",
    "\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "#datayı okuduk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28*28=784 input sayısı, 50 lik nodelardan oluşan 2 hidden layer kullanacağız. output layer zaten 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=784\n",
    "output_size=10\n",
    "hidden_layer_size=50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#bu kod önceki run dan hafızada kalan değerleri resetlemek için kullanılır\n",
    "#w , b değerlerini güncelledikçe tekrar çalıştıracağımızdan buna ihtiyaç var.\n",
    "\n",
    "inputs=tf.placeholder(tf.float32,[None,input_size])\n",
    "targets=tf.placeholder(tf.float32,[None,output_size])\n",
    "\n",
    "#3 tane w ve b değeri olduğundan(2 hidden unit ve input output araları) ayrı ayrı giriyoruz\n",
    "weights_1=tf.get_variable(\"weights_1\",[input_size,hidden_layer_size]) #size ı 784*50\n",
    "biases_1=tf.get_variable(\"biases_1\",[hidden_layer_size])#size ı 50\n",
    "#değişkenleri atamanın geleneksel bir yoludur bu. ayrıca default olduğundan Xavier initializer kullanır.\n",
    "#name ve shape parametreleridir\n",
    "\n",
    "#burada hidden layer 1 değerlerini buluyoruz\n",
    "outputs_1=tf.nn.relu(tf.matmul(inputs,weights_1)+biases_1) # relu aktivasyon fonksiyonunu kullandık.(nonliearity)\n",
    "#tf.nn neural network ün kısaltması. yaygın kullanılan aktivasyonn fonksiyonlarını barındırır.\n",
    "\n",
    "#1. işlem tamam şimdi sıra 2.de\n",
    "\n",
    "weights_2=tf.get_variable(\"weights_2\",[hidden_layer_size,hidden_layer_size])\n",
    "biases_2=tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "\n",
    "outputs_2=tf.nn.relu(tf.matmul(outputs_1,weights_2)+biases_2)\n",
    "\n",
    "weights_3=tf.get_variable(\"weights_3\", [hidden_layer_size,output_size])\n",
    "biases_3=tf.get_variable(\"biases_3\",[output_size])\n",
    "\n",
    "outputs=tf.matmul(outputs_2,weights_3)+biases_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Training loss:  0.410. Validation loss:  0.208. Validation accuracy: 94.02%\n",
      "Epoch: 2. Training loss:  0.187. Validation loss:  0.160. Validation accuracy: 95.40%\n",
      "Epoch: 3. Training loss:  0.142. Validation loss:  0.133. Validation accuracy: 96.10%\n",
      "Epoch: 4. Training loss:  0.117. Validation loss:  0.123. Validation accuracy: 96.12%\n",
      "Epoch: 5. Training loss:  0.099. Validation loss:  0.121. Validation accuracy: 96.28%\n",
      "Epoch: 6. Training loss:  0.085. Validation loss:  0.105. Validation accuracy: 96.70%\n",
      "Epoch: 7. Training loss:  0.076. Validation loss:  0.105. Validation accuracy: 96.84%\n",
      "Epoch: 8. Training loss:  0.066. Validation loss:  0.107. Validation accuracy: 97.02%\n",
      "overfitting!!!!!!!\n",
      "Training sonu\n"
     ]
    }
   ],
   "source": [
    "#outputumuz var ama bunu aktivasyon fonksiyonuna sokmadık. output layerda softmax kullanıyoruz\n",
    "#bu fonksiyon cross entropy loss function kullanıyor bu şekilde.\n",
    "#bu fonksiyon çok küçük sayılarla uğraşıldığında kullanılmalıdır kullanılmazda modeli bozar\n",
    "#bizim bu modelimizde gerekir mi bilinmez ama kullanmak güvenli olur\n",
    "#logits ayarlanmamış ihtimaller demektir.yani vektörün softmaxa girmeden önceki hali.softmax bunları ihtimal formatına dönüştürür\n",
    "#fonksiyon 2 parametre alır: logits,labels\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs,labels=targets)\n",
    "\n",
    "#loss var ama mean loss ile çalışmak performansı artırır\n",
    "#kullanılan fonksiyon boyutlar halindeki tensorun ortalamasını bulur\n",
    "\n",
    "mean_loss=tf.reduce_mean(loss)\n",
    "\n",
    "#optimizason fonksiyonu belirlenir.öncekinde gd kullanmıştık ama artık adam kullanacağız\n",
    "\n",
    "optimize= tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)\n",
    "\n",
    "#output ile targetı karşılaştırıyoruz\n",
    "#tf.equal eşit mi değil mi değeri döndürür t/f\n",
    "#tf.argmax fonksiyonu içine verilen değer matrisinin neresinin karşılaştırılacağını alır.outpus,1 derse output matrisinin sütunu, output,0 derse satırı olur\n",
    "#out_equals_target da her gözlem için 0-1 lerden oluşan bir vektördür.\n",
    "#accuracy ise bu out_equals_target vektörünün meanidir.bu vektör boolean olduğundan sonucta hata verdirebilir. onun için bunu tf.cast ile floata çeviriyoruz\n",
    "#tf.cast(object,data_type) bir objeyi başka bir data tipine dönüştürmeye yarar\n",
    "\n",
    "out_equals_target=tf.equal(tf.argmax(outputs,1),tf.argmax(targets,1))\n",
    "\n",
    "accuracy=tf.reduce_mean(tf.cast(out_equals_target,tf.float32))\n",
    "# bu iki satır modelin training accuracy değerini 0-1 arasında olacak şekilde hesapladı.\n",
    "\n",
    "#session \n",
    "sess=tf.InteractiveSession()\n",
    "initializer=tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "##### BATCHİNG ##########\n",
    "#batch size =1 olursa SGD olmuş olur. Batch size= number of samples olursa da GD olur.\n",
    "\n",
    "batch_size=100 #örnek sayısını batch size a bölünce batch sayısı çıkar\n",
    "batches_number=mnist.train.num_examples // batch_size # // tam sayı bölme\n",
    "\n",
    "\n",
    "########## EARLY STOPPING MEKANİZMALARI########\n",
    "\n",
    "#ilk olarak algoritmanın train edeceği maximum epoch sayısını giriyoruz\n",
    "#early stopping eğer validation loss artıyorsa \n",
    "\n",
    "max_epochs=15 # makul zamanda bitirmesi için 15 girdik\n",
    "prev_validation_loss= 9999999 #yeterince büyük verdik ki 1. epochta hemen bitmesin diye\n",
    "\n",
    "###### LEARNING #############\n",
    "# her epochta batches kendi içinde tekrar işlem yapacağından loop içinde loop yaptık\n",
    "\n",
    "for epoch_counter in range(max_epochs):\n",
    "    #başlangıçta loss değerini 0 layacağız. ancak herhangi birşey kaybolmayacak \n",
    "    #çünkü herşey parametrelerin backpropagate edilmesiyle geri gelecek\n",
    "    \n",
    "    curr_epoch_loss=0.\n",
    "    \n",
    "    for batch_counter in range(batches_number):\n",
    "        \n",
    "        input_batch, target_batch=mnist.train.next_batch(batch_size)\n",
    "        #100 input 100 target yükledi\n",
    "        #mnist data setini import ederken, datayı bir batchten diğerine yükleyen bir fonksiyonu daha import etmiş oluyoruz. Bu fonksiyon next_batch.\n",
    "        #yukarıdaki işlemle input batch ve target batch basitçe yüklemiş olduk\n",
    "        \n",
    "        #şimdi session run işlemi yaparak lossu optimize ve hesaplama yapcaz.\n",
    "        _, batch_loss=sess.run([optimize,mean_loss],\n",
    "                              feed_dict={inputs:input_batch,targets:target_batch})\n",
    "        #bu 100 örneği kullanarak optimizasyon yaptı\n",
    "    \n",
    "        curr_epoch_loss+=batch_loss\n",
    "        #loss değerini kaydetti\n",
    "        #tekrar başa döndüğünde bir sonraki 100 örneği yükler.Trainin data seti bitene kadar bunu tekrarlar\n",
    "        \n",
    "    #ortalama curr_epoch loss bulunur. bu şuanda mean_loss oldu.bu aynı zamanda trainin loss olmuş oldu\n",
    "    \n",
    "    curr_epoch_loss/=batches_number \n",
    "    \n",
    "    #her epochtan sonra validate ettiğimizden validation loss bulunur\n",
    "    #validation yaparken sadece forward propagate yapıyorduk. bunu yaparken validation seti kullanırız\n",
    "    \n",
    "    input_batch,target_batch=mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "    \n",
    "    #forward propagate yaparken val loss ve val accuracy değerini kaydetmemiz gerekir.\n",
    "    \n",
    "    validation_loss,validation_accuracy=sess.run([mean_loss,accuracy],\n",
    "                                                feed_dict={inputs:input_batch,targets:target_batch})\n",
    "    #bu işlemle validation data setinin mean_loss ve accuracy değerini hesapladık ve kaydettik    \n",
    "    \n",
    "    ######## YAZDIRMA ############\n",
    "    \n",
    "    print(\"Epoch: \" +str(epoch_counter+1)+\n",
    "          \". Training loss: \"+\" {0:.3f}\".format(curr_epoch_loss)+\n",
    "          \". Validation loss: \"+\" {0:.3f}\".format(validation_loss)+\n",
    "          \". Validation accuracy: \"+\"{0:.2f}\".format(validation_accuracy*100.)+\"%\"\n",
    "         )\n",
    "    \n",
    "    \n",
    "    #### EARLY STOPPING MEKANİZMASI #########\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        print(\"overfitting!!!!!!!\")\n",
    "        break\n",
    "        \n",
    "    prev_validation_loss=validation_loss #böylece validation loss artmaya başlayınca duracak\n",
    "    \n",
    "print(\"Training sonu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu değerler modelin accuracy eğeri değil. Modelin accuracy değeri test setiyle forward propagate yapınca çıkan sonuçtur. Validation accuracy değerini hiperparametrelerle, aktivasyon fonksiyonuyla, vs oynayarak sürekli geliştirmeye çalışırız. 10-20 denemeden sonra en doğru validation accuracy değerini buluruz. Bunu bulurken hiperparametrelerin en optimum değerlerini bulmuş OLMAYIZ. Biz sadece validation dataset için optimum hiperpar leri bulmuş oluruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "input_batch,target_batch=mnist.test.next_batch(mnist.test._num_examples)\n",
    "test_accuracy=sess.run([accuracy], feed_dict={inputs:input_batch,targets:target_batch})\n",
    "\n",
    "test_accuracy_percent = test_accuracy[0]*100.\n",
    "# run ile bir single output yürütürsek sess.run sonucu tek elemanlı list olur.\n",
    "#Burada da o list deki tek elemanı alarak çarptık 100 le\n",
    "\n",
    "print(\"Test accuracy: \" + \"{0:.2f}\".format(test_accuracy_percent)+\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZAMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelin çalışma süresi: 12.432711124420166 \n"
     ]
    }
   ],
   "source": [
    "bitis=time.time()\n",
    "print(\"Modelin çalışma süresi: %s \" %(bitis-baslangic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
